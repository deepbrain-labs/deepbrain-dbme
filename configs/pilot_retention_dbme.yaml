trial_id: "pilot_retention_dbme_001"
seed: 0
output_dir: "results/pilot_dbme"

model:
  name: "gpt2"
  max_length: 512
  language_model:
    input_dim: 768
    hidden_dim: 768
  hippocampal_encoder:
    input_dim: 768
    slot_dim: 256
    key_dim: 128
  router:
    input_dim: 768
    mode: "learned"
    warm_start_steps: 1000
  consolidation:
    mode: "prototype"
    frequency: 25
    proto_k: 256
    rehearsal_r: 10
  retrieval_k: 8

storage:
  episodic_store:
    capacity: 1000
    eviction_policy: "importance_age"
  k_store: {}

training:
  batch_size: 1
  learning_rate: 2e-5
  epochs: 1
  device: "cuda"

metrics:
  - "retention_at_k"
  - "memory_bytes"

faiss:
  dimension: 128
  index_type: "Flat"
  metric: "inner_product"
